{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88bbd2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "\n",
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6191bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa1fc7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 6,285,452\n",
      "Total words: 1,101,476\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "book_files = glob.glob('./data/*.txt')\n",
    "\n",
    "# combine all books into one text\n",
    "all_text = \"\"\n",
    "for file_path in sorted(book_files):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        all_text += f.read() + \"\\n\\n\"\n",
    "    \n",
    "print(f\"Total characters: {len(all_text):,}\")\n",
    "print(f\"Total words: {len(all_text.split()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e86f3550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1669304 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 6519\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def create_chunks(all_text, chunk_size=512, stride=256):\n",
    "    tokens = tokenizer.encode(all_text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(tokens) - chunk_size, stride):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "chunks = create_chunks(all_text, chunk_size=512, stride=256)\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b77c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to labels(for causal LM, inputs=labels)\n",
    "texts = [tokenizer.decode(chunk, skip_special_tokens=False) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a514a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 6519\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataset\n",
    "dataset = Dataset.from_dict({\"text\": texts})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c669638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6519/6519 [00:06<00:00, 943.33 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 6519\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize properly for training\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ea7292a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6519/6519 [00:00<00:00, 9596.12 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 6519\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for causal LM, labels = input_ids\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    lambda x: {\"labels\": x[\"input_ids\"]},\n",
    "    batched=True\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80c110b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5867\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 652\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/val split\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b67cf7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "768356b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    bias=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5628f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 125,029,632 || trainable%: 0.4717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akash/miniconda3/envs/ddods/lib/python3.12/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "699d7800",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = split_dataset[\"train\"].select(range(50))\n",
    "small_eval = split_dataset[\"test\"].select(range(10))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./harry-potter-lora\",\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_eval,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d14ccb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akash/miniconda3/envs/ddods/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=3.4894612630208335, metrics={'train_runtime': 41.0501, 'train_samples_per_second': 3.654, 'train_steps_per_second': 0.292, 'total_flos': 39465595699200.0, 'train_loss': 3.4894612630208335, 'epoch': 3.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "871a75c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./harry-potter-lora-final/tokenizer_config.json',\n",
       " './harry-potter-lora-final/special_tokens_map.json',\n",
       " './harry-potter-lora-final/vocab.json',\n",
       " './harry-potter-lora-final/merges.txt',\n",
       " './harry-potter-lora-final/added_tokens.json')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./harry-potter-lora-final')\n",
    "tokenizer.save_pretrained(\"./harry-potter-lora-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1e74786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model = PeftModel.from_pretrained(base_model, \"./harry-potter-lora-final\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./harry-potter-lora-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c01657f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: The boy with the lightning scar\n",
      "==================================================\n",
      "Base GPT-2:\n",
      "The boy with the lightning scar was in a state of shock and confusion.\n",
      "It wasn't until later that he realized what had happened, so his mother took him to hospital for treatment. The wound has healed nicely since then. He is recovering well but needs all kinds from other injuries as an adult – most notably torn muscles (the ones found on those who have been attacked by monsters). Now we know about how it started: this kid came out after being beaten up at school once because there were more than one person involved or maybe just some bullies yelling 'I'm not you' when they heard my voice telling them I got beat down before running off! It's funny enough where kids start showing signs of aggression like walking around alone without taking care\n",
      "\n",
      "LoRA Fine-tuned:\n",
      "The boy with the lightning scar from his foot had no idea what he was looking at, but when a doctor asked about it to him and they saw that there were two little black holes in this image as seen by God on Earth (see chapter 2), Jesus began to cry out. Then He suddenly took off all of those blue lights which have since been taken away for their own reasons:\n",
      "\n",
      "Jesus said to me \"If you don't take them down yet I will not let any more people enter into my house.\" The doctors came forward saying, 'Do we see these rays?' And if anyone did say yes then everyone would be healed… They also heard one voice crying aloud…. So whoever has received an angelic message is being saved now…\" [John 8:20]\n",
      "\n",
      "\n",
      "\"Now before going up toward heaven,\" Christ once again told Him through His disciples Mary Magdalene and Peter who are already standing outside after receiving some spiritual light; so even though our understanding does not quite\n",
      "\n",
      "Prompt: The magical castle\n",
      "==================================================\n",
      "Base GPT-2:\n",
      "The magical castle that has stood here since the beginning of time.\n",
      "\n",
      "You will not be able to enter through its gates as long you do this right now, so don't worry about it.\"\n",
      ": A woman was waiting for me in front from outside and her hair flowed all over my face like a waterfall… And she had an extremely cute expression on her cheeks! I really felt jealous because after being asked what kind person would come out today but if there's someone who looks down at us then we could definitely get into trouble!! But just when i thought \"this place is weird\", suddenly seeing them walking towards our room looked scary too?! They came inside immediately with no one saying anything or any sign whatsoever.. Even though they were standing before\n",
      "\n",
      "LoRA Fine-tuned:\n",
      "The magical castle is about as far away from home in Scotland and England.\n",
      ": You can look back on the time that I spent at Hogwarts with my sister, Hermione Granger.\"\n",
      "\n",
      " (Image Source)\n",
      "\n",
      "Prompt: He opened the dusty book\n",
      "==================================================\n",
      "Base GPT-2:\n",
      "He opened the dusty book and read. It was a letter from an old lady, who had taken her daughter to school when she came across this girl in bed with two young boys called Daphne (in Hebrew: לָּעבו) . She felt that they were all very handsome of age but also didn't know how their father could even marry such girls as he did not want them for him yet – so his sister went down on one side while telling about what happened at first sight - I think it would be better if we should go out together because there are some people here whose names have been mentioned by others,\" Kishor said calmly.\"And then suddenly you heard these words:\"I will look after your\n",
      "\n",
      "LoRA Fine-tuned:\n",
      "He opened the dusty book. \"I'm a novelist, too.\"\n",
      "\"And that's what you do,\" I said without hesitation. Then he read aloud: \"In my career as an actor, not so much in comedy—that is to say it was never part of your job description or even on-screen work!\" He didn't know whether this would come back later; but now and then there'd be some kind words coming up from him when they came out for me like 'You must have heard about these things.' It seemed good if anyone had seen them before! But after three years his body relaxed again—\"he looked very tired at home\"—and we talked more slowly down here than our conversation could possibly last eight minutes each day (my own family has two wives who are journalists.) When asked why she wrote those lines today, Echols told her simply : \"That doesn' t make any sense anymore . My husband went off into Mexico with us during World War II because\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=200, temperature=0.8, top_p=0.9):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def generate_text_base(prompt, max_length=150, temperature=0.8, top_p=0.9):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = base_model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"The boy with the lightning scar\",\n",
    "    \"The magical castle\",\n",
    "    \"He opened the dusty book\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Base GPT-2:\")\n",
    "    print(generate_text_base(prompt))\n",
    "    print(\"\\nLoRA Fine-tuned:\")\n",
    "    print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18c103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddods",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
